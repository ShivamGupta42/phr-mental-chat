Commands 
1. huggingface-cli delete-cache
2. docker build -t {image_name} .
3. docker run --gpus all -p 6006:6006 -itd --name "phr-chat-ai"Â c366b6937df4
4. HuggingFace TGI is a bit buggged, we need to change content.strip() to content in tokenizer_config.json
5. For creating dev container we use "dockerfile-dev-env" to create an image using 2 , and run the container using 3.
5.1 We have added the requirements.txt to github gist and use it above.



Observations:
1. LLaMA-2, doesn't predict spaces " " in the output, in output ids, generated by the model.generate()
2. LLaMA-2, tokeniser removes the spaces while tokenising so most of the times space is not present in the tokenised ids.
3. tokenizer.decode() adds spaces manually, taking care of the special tokens and punctuation marks.