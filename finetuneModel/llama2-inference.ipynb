{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer,\n",
    "                          BitsAndBytesConfig, HfArgumentParser,\n",
    "                          TrainingArguments, logging, pipeline)\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "new_model_name = \"llama-2-7b-chat-hf-phr_mental_therapy-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_4bit=True\n",
    "device_map = {\"\": 0}\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLoRA Inference with adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(new_model_name)\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "model = PeftModel.from_pretrained(base_model, new_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text generation using model.generate\n",
    "conv = [ { \"content\": \"You are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\", \"role\": \"system\" } ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(conv,model,tokenizer):\n",
    "    # if we set tokenize=False, only the chat template is applied.\n",
    "    # no tokenization is done, and we get string insted of token ids.\n",
    "    prompt = tokenizer.apply_chat_template(conv,tokenize=False)\n",
    "    # THe model.generate() takes token input_ids as input and returns output_ids.\n",
    "    # For Training we need both input_ids and attention_mask using tokenizer()\n",
    "    # and not tokenizer.encode() as it returns only input_ids.\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.to('cuda')\n",
    "    output_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=1024,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    top_k=60,\n",
    "    temperature=0.9)\n",
    "    output_text = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_model(user_input,conv,model,tokenizer):\n",
    "    conv.append({\"content\": user_input, \"role\": \"user\"})\n",
    "    response = generate_response(conv,model,tokenizer)\n",
    "    print(\"Model:\", response)\n",
    "    conv.append({\"content\": response, \"role\": \"assistant\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_with_model(\"Hi\",conv,model,tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
