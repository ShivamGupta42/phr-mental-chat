{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict, load_metric,load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We must tokenise the input before feeding to the model. We use the tokenizer from the pretrained model.\n",
    "- The dataset must have the label column named as 'label', to be trained using Trainer API. It is preferable to keep the text column as 'text'. However since we are tokenising the text column, it can be named anything, as we just pass the tokenised columns input_ids and attention_mask to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for huggingface pipeline to give text labels instead of numbers\n",
    "id2label = {0: \"suicide\", 1:\"non-suicide\"}\n",
    "label2id = {\"suicide\":0, \"non-suicide\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model_name = \"PHR_Suicide_Prediction_Roberta_Cleaned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2,label2id=label2id,id2label=id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"vibhorag101/roberta-base-suicide-prediction-phr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniseDataset(dataset):\n",
    "    return(tokeniser(dataset[\"text\"],padding=\"max_length\",truncation=True))\n",
    "\n",
    "def convertLabel2ID(dataset):\n",
    "    dataset['label'] = label2id[dataset['label']]\n",
    "    return dataset\n",
    "    \n",
    "dataset = dataset.map(convertLabel2ID) \n",
    "tokenisedDataset = dataset.map(tokeniseDataset,batched=True)\n",
    "\n",
    "trainTokeniseDataset = tokenisedDataset[\"train\"]\n",
    "tempTokenisedDataset= tokenisedDataset[\"test\"]\n",
    "tempTokenisedDataset = tempTokenisedDataset.train_test_split(test_size=0.5)\n",
    "testTokenisedDataset = tempTokenisedDataset[\"train\"]\n",
    "valTokenisedDataset = tempTokenisedDataset[\"test\"]\n",
    "# print(trainTokeniseDataset)\n",
    "# print(valTokenisedDataset)\n",
    "# print(testTokenisedDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    metric_acc = evaluate.load(\"accuracy\")\n",
    "    metric_rec = evaluate.load(\"recall\")\n",
    "    metric_pre = evaluate.load(\"precision\")\n",
    "    metric_f1 = evaluate.load(\"f1\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = metric_acc.compute(predictions=predictions, references=labels)\n",
    "    recall = metric_rec.compute(predictions=predictions, references=labels)\n",
    "    precision = metric_pre.compute(predictions=predictions, references=labels)\n",
    "    f1 = metric_f1.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"recall\": recall, \"precision\": precision, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "wandb.init(project=\"huggingface\", entity=\"vibhor20349\", name=finetune_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=finetune_model_name,\n",
    "    report_to = 'wandb',\n",
    "    learning_rate=2e-5, # recommended in roberta paper\n",
    "    num_train_epochs=3, #recommended in bert paper\n",
    "    per_device_train_batch_size=16, # recommended in roberta paper\n",
    "    per_device_eval_batch_size=16, # recommended in roberta paper\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps = 1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True #to clone the training repo just before starting to train, so push to hub works\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model= model,\n",
    "    args=training_args,\n",
    "    train_dataset=trainTokeniseDataset,\n",
    "    eval_dataset=valTokenisedDataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokeniser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(trainer.predict(testTokenisedDataset).metrics)\n",
    "trainer.evaluate(eval_dataset=testTokenisedDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "#trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(finetune_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(finetune_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IF we want to make predictions\n",
    "input_text = \"Your input text goes here.\"\n",
    "inputs = tokeniser(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    ## BERT expects the keyword agruments \"token_type_ids\" and \"attention_mask\" in input.\n",
    "    # so we convert inputs dictionary to keyword arguments using ** before passing to the model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract the predicted probabilities or labels\n",
    "logits = outputs.logits\n",
    "probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
